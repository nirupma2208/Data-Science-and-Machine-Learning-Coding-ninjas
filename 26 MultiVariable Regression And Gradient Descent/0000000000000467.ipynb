{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBJzxfQ85wTT"
   },
   "source": [
    "## **Limitations of Linear Regression**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jj8SkGwP54Al"
   },
   "source": [
    "Linear regression, though a very powerful algorithm, has certain disadvantages\n",
    "1. **Main limitation of Linear Regression** is the assumption of linearity between the dependent variable and the independent variables. In the real world, the data is almost never linearly separable. The assumption that there is a straight line relationship is usually wrong.\n",
    "\n",
    "2. **Prone to noise and overfitting:** If the number of observations are lesser than the number of features, Linear Regression should not be used, otherwise it may lead to overfit, and the relationship thus formed will be noisy.\n",
    "\n",
    "3. **Prone to outliers:** Linear regression is very sensitive to outliers. An outlier can be considered as an anomaly. It refers to a datapoint which has no clear relationship with any other data point in the data. So, outliers should be analyzed and removed before applying Linear Regression to the dataset, or the linear relationship formed would be highly skewed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxKroilG54Ux"
   },
   "source": [
    "Let's explore the first limitation in detail. Have a look at the following graph:\n",
    "\n",
    "<img src=\"https://files.codingninjas.in/graph_gd-7051.jpg\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ny2szGF354lL"
   },
   "source": [
    "In the above figure, it is quite clear, that the linear line formed will not correctly predict the results of data points(shown in blue).\n",
    "\n",
    "Thus, we need to plot more complex boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUvopCtz542q"
   },
   "source": [
    "## **Plotting More Complex Boundaries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecsppjFO55H-"
   },
   "source": [
    "As we have learnt in the previous modules, **Function or Hypothesis** of Linear Regression is represented by -\n",
    "$y = m_1.x_1 + m_2.x_2 + m_3.x_3 + … + m_n.x_n + b$.\n",
    "This is essentially a line of the form $ y = mx + c$.\n",
    "\n",
    "To plot more complex boundaries, we need to have an equation of higher degree. For example:\n",
    "1. $ y = m_1x^2 + m_2x + c $\n",
    "2. $ y = m_1x^3 + m_2x^2 + m_3x + c$\n",
    "\n",
    "So, the basic idea to do this, is to add dummy features in our dataset. Suppose the current data set has three features: $x_1, x_2 $ and $x_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pqcPdx4_sUF"
   },
   "source": [
    "$x_1$ | $x_2$ | $x_3$\n",
    "--- | --- | ---\n",
    "1 | 3 | 5 \n",
    "2 | 4 | 6 \n",
    "- | - | - \n",
    "- | - | - \n",
    "- | - | - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5Xb3R6rAqE9"
   },
   "source": [
    "The equation for the above dataset would be :\n",
    "\n",
    "$y = m_1.x_1 + m_2.x_2 + m_3.x_3 + b$.\n",
    "\n",
    "We can add dummy features to our dataset by enhancing the already existing features. One of the most common method is to create a feature, which is the product of already existing features. Let us create a new feature, $x_{12}$ which is the product of $x_1$ and $x_2$. So, our data set becomes :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YD5w1abBeca"
   },
   "source": [
    "$x_1$ | $x_2$ | $x_3$ | $x_{12}$\n",
    "--- | --- | --- | ---\n",
    "1 | 3 | 5 | $\\;$3\n",
    "2 | 4 | 6 | $\\;$8\n",
    "- | - | - | $\\;$- \n",
    "- | - | - | $\\;$- \n",
    "- | - | - | $\\;$- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYFEXre8CJEZ"
   },
   "source": [
    "The equation for the above dataset would be :\n",
    "\n",
    "$y = m_1.x_1 + m_2.x_2 + m_3.x_3 + m_4.x_{12}+ b$.\n",
    "\n",
    "where the term $x_{12}$ is essentially a quadratic term.\n",
    "\n",
    "We can add more features like $x_{31}$ and $x_{23}$. And we are not just limited to quadratic terms, we may also add cubic terms like $x_{123}$.\n",
    "Other logarithimic and trignometric functions may also be used to plot more complex boundaries.\n",
    "\n",
    "Let's look at an example of how to add a feature into our dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfCv-1GWD4Je"
   },
   "source": [
    "### **Example on how to code complex boundaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9JvaGqJXEIno"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v1H2PYY6DI2W",
    "outputId": "36b8b8f9-f599-4caa-c16d-daae89a0bcdd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   8.58,   38.38, 1021.03,   84.37],\n",
       "       [  21.79,   58.2 , 1017.21,   66.74],\n",
       "       [  16.64,   48.92, 1011.55,   78.76],\n",
       "       ...,\n",
       "       [  29.8 ,   69.34, 1009.36,   64.74],\n",
       "       [  16.37,   54.3 , 1017.94,   63.63],\n",
       "       [  30.11,   62.04, 1010.69,   47.96]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.loadtxt(\"https://files.codingninjas.in/0000000000002419_training_ccpp_x_y_train-7050.csv\", delimiter=\",\")\n",
    "x = train_data[:,:-1]\n",
    "y = train_data[:,-1]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7176,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "0IosJ1-bEPyK",
    "outputId": "cc9815e7-de0e-454f-a57f-dd2546d5c5ad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.58</td>\n",
       "      <td>38.38</td>\n",
       "      <td>1021.03</td>\n",
       "      <td>84.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.79</td>\n",
       "      <td>58.20</td>\n",
       "      <td>1017.21</td>\n",
       "      <td>66.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.64</td>\n",
       "      <td>48.92</td>\n",
       "      <td>1011.55</td>\n",
       "      <td>78.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.38</td>\n",
       "      <td>71.32</td>\n",
       "      <td>1009.17</td>\n",
       "      <td>60.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.20</td>\n",
       "      <td>40.03</td>\n",
       "      <td>1017.05</td>\n",
       "      <td>92.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7171</th>\n",
       "      <td>9.32</td>\n",
       "      <td>37.73</td>\n",
       "      <td>1022.14</td>\n",
       "      <td>79.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7172</th>\n",
       "      <td>11.20</td>\n",
       "      <td>41.38</td>\n",
       "      <td>1021.65</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7173</th>\n",
       "      <td>29.80</td>\n",
       "      <td>69.34</td>\n",
       "      <td>1009.36</td>\n",
       "      <td>64.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7174</th>\n",
       "      <td>16.37</td>\n",
       "      <td>54.30</td>\n",
       "      <td>1017.94</td>\n",
       "      <td>63.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7175</th>\n",
       "      <td>30.11</td>\n",
       "      <td>62.04</td>\n",
       "      <td>1010.69</td>\n",
       "      <td>47.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7176 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1     x2       x3     x4\n",
       "0      8.58  38.38  1021.03  84.37\n",
       "1     21.79  58.20  1017.21  66.74\n",
       "2     16.64  48.92  1011.55  78.76\n",
       "3     31.38  71.32  1009.17  60.42\n",
       "4      9.20  40.03  1017.05  92.46\n",
       "...     ...    ...      ...    ...\n",
       "7171   9.32  37.73  1022.14  79.49\n",
       "7172  11.20  41.38  1021.65  61.89\n",
       "7173  29.80  69.34  1009.36  64.74\n",
       "7174  16.37  54.30  1017.94  63.63\n",
       "7175  30.11  62.04  1010.69  47.96\n",
       "\n",
       "[7176 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = ['x1','x2','x3','x4']\n",
    "df = pd.DataFrame(x)\n",
    "df.columns = li\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "73wPMTRmEPm_",
    "outputId": "f74e4681-6e57-4b0d-c052-bedb86468017"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x1_x1</th>\n",
       "      <th>x1_x2</th>\n",
       "      <th>x1_x3</th>\n",
       "      <th>x1_x4</th>\n",
       "      <th>x2_x2</th>\n",
       "      <th>x2_x3</th>\n",
       "      <th>x2_x4</th>\n",
       "      <th>x3_x3</th>\n",
       "      <th>x3_x4</th>\n",
       "      <th>x4_x4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.58</td>\n",
       "      <td>38.38</td>\n",
       "      <td>1021.03</td>\n",
       "      <td>84.37</td>\n",
       "      <td>73.6164</td>\n",
       "      <td>329.3004</td>\n",
       "      <td>8760.4374</td>\n",
       "      <td>723.8946</td>\n",
       "      <td>1473.0244</td>\n",
       "      <td>39187.1314</td>\n",
       "      <td>3238.1206</td>\n",
       "      <td>1.042502e+06</td>\n",
       "      <td>86144.3011</td>\n",
       "      <td>7118.2969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.79</td>\n",
       "      <td>58.20</td>\n",
       "      <td>1017.21</td>\n",
       "      <td>66.74</td>\n",
       "      <td>474.8041</td>\n",
       "      <td>1268.1780</td>\n",
       "      <td>22165.0059</td>\n",
       "      <td>1454.2646</td>\n",
       "      <td>3387.2400</td>\n",
       "      <td>59201.6220</td>\n",
       "      <td>3884.2680</td>\n",
       "      <td>1.034716e+06</td>\n",
       "      <td>67888.5954</td>\n",
       "      <td>4454.2276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.64</td>\n",
       "      <td>48.92</td>\n",
       "      <td>1011.55</td>\n",
       "      <td>78.76</td>\n",
       "      <td>276.8896</td>\n",
       "      <td>814.0288</td>\n",
       "      <td>16832.1920</td>\n",
       "      <td>1310.5664</td>\n",
       "      <td>2393.1664</td>\n",
       "      <td>49485.0260</td>\n",
       "      <td>3852.9392</td>\n",
       "      <td>1.023233e+06</td>\n",
       "      <td>79669.6780</td>\n",
       "      <td>6203.1376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.38</td>\n",
       "      <td>71.32</td>\n",
       "      <td>1009.17</td>\n",
       "      <td>60.42</td>\n",
       "      <td>984.7044</td>\n",
       "      <td>2238.0216</td>\n",
       "      <td>31667.7546</td>\n",
       "      <td>1895.9796</td>\n",
       "      <td>5086.5424</td>\n",
       "      <td>71974.0044</td>\n",
       "      <td>4309.1544</td>\n",
       "      <td>1.018424e+06</td>\n",
       "      <td>60974.0514</td>\n",
       "      <td>3650.5764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.20</td>\n",
       "      <td>40.03</td>\n",
       "      <td>1017.05</td>\n",
       "      <td>92.46</td>\n",
       "      <td>84.6400</td>\n",
       "      <td>368.2760</td>\n",
       "      <td>9356.8600</td>\n",
       "      <td>850.6320</td>\n",
       "      <td>1602.4009</td>\n",
       "      <td>40712.5115</td>\n",
       "      <td>3701.1738</td>\n",
       "      <td>1.034391e+06</td>\n",
       "      <td>94036.4430</td>\n",
       "      <td>8548.8516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7171</th>\n",
       "      <td>9.32</td>\n",
       "      <td>37.73</td>\n",
       "      <td>1022.14</td>\n",
       "      <td>79.49</td>\n",
       "      <td>86.8624</td>\n",
       "      <td>351.6436</td>\n",
       "      <td>9526.3448</td>\n",
       "      <td>740.8468</td>\n",
       "      <td>1423.5529</td>\n",
       "      <td>38565.3422</td>\n",
       "      <td>2999.1577</td>\n",
       "      <td>1.044770e+06</td>\n",
       "      <td>81249.9086</td>\n",
       "      <td>6318.6601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7172</th>\n",
       "      <td>11.20</td>\n",
       "      <td>41.38</td>\n",
       "      <td>1021.65</td>\n",
       "      <td>61.89</td>\n",
       "      <td>125.4400</td>\n",
       "      <td>463.4560</td>\n",
       "      <td>11442.4800</td>\n",
       "      <td>693.1680</td>\n",
       "      <td>1712.3044</td>\n",
       "      <td>42275.8770</td>\n",
       "      <td>2561.0082</td>\n",
       "      <td>1.043769e+06</td>\n",
       "      <td>63229.9185</td>\n",
       "      <td>3830.3721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7173</th>\n",
       "      <td>29.80</td>\n",
       "      <td>69.34</td>\n",
       "      <td>1009.36</td>\n",
       "      <td>64.74</td>\n",
       "      <td>888.0400</td>\n",
       "      <td>2066.3320</td>\n",
       "      <td>30078.9280</td>\n",
       "      <td>1929.2520</td>\n",
       "      <td>4808.0356</td>\n",
       "      <td>69989.0224</td>\n",
       "      <td>4489.0716</td>\n",
       "      <td>1.018808e+06</td>\n",
       "      <td>65345.9664</td>\n",
       "      <td>4191.2676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7174</th>\n",
       "      <td>16.37</td>\n",
       "      <td>54.30</td>\n",
       "      <td>1017.94</td>\n",
       "      <td>63.63</td>\n",
       "      <td>267.9769</td>\n",
       "      <td>888.8910</td>\n",
       "      <td>16663.6778</td>\n",
       "      <td>1041.6231</td>\n",
       "      <td>2948.4900</td>\n",
       "      <td>55274.1420</td>\n",
       "      <td>3455.1090</td>\n",
       "      <td>1.036202e+06</td>\n",
       "      <td>64771.5222</td>\n",
       "      <td>4048.7769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7175</th>\n",
       "      <td>30.11</td>\n",
       "      <td>62.04</td>\n",
       "      <td>1010.69</td>\n",
       "      <td>47.96</td>\n",
       "      <td>906.6121</td>\n",
       "      <td>1868.0244</td>\n",
       "      <td>30431.8759</td>\n",
       "      <td>1444.0756</td>\n",
       "      <td>3848.9616</td>\n",
       "      <td>62703.2076</td>\n",
       "      <td>2975.4384</td>\n",
       "      <td>1.021494e+06</td>\n",
       "      <td>48472.6924</td>\n",
       "      <td>2300.1616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7176 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1     x2       x3  ...         x3_x3       x3_x4      x4_x4\n",
       "0      8.58  38.38  1021.03  ...  1.042502e+06  86144.3011  7118.2969\n",
       "1     21.79  58.20  1017.21  ...  1.034716e+06  67888.5954  4454.2276\n",
       "2     16.64  48.92  1011.55  ...  1.023233e+06  79669.6780  6203.1376\n",
       "3     31.38  71.32  1009.17  ...  1.018424e+06  60974.0514  3650.5764\n",
       "4      9.20  40.03  1017.05  ...  1.034391e+06  94036.4430  8548.8516\n",
       "...     ...    ...      ...  ...           ...         ...        ...\n",
       "7171   9.32  37.73  1022.14  ...  1.044770e+06  81249.9086  6318.6601\n",
       "7172  11.20  41.38  1021.65  ...  1.043769e+06  63229.9185  3830.3721\n",
       "7173  29.80  69.34  1009.36  ...  1.018808e+06  65345.9664  4191.2676\n",
       "7174  16.37  54.30  1017.94  ...  1.036202e+06  64771.5222  4048.7769\n",
       "7175  30.11  62.04  1010.69  ...  1.021494e+06  48472.6924  2300.1616\n",
       "\n",
       "[7176 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    for j in range(i, 4):\n",
    "        ele = li[i] + \"_\" + li[j]\n",
    "        df[ele] = df[li[i]]*df[li[j]]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Dzz9gAlElSS"
   },
   "source": [
    "In the above dataset, we had 4 existing features. Using them, we added 10 dummy features who have a quadratic degree. Similar methods can be used to add cubic degree too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8opjAEwpCzkk"
   },
   "source": [
    "**But remember! Don't try to add many extra dummy features, as it leads to overfitting the data, leading to incorrect results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUWTYPHJGSzF"
   },
   "source": [
    "Optimization is a big part of machine learning. Almost every machine learning algorithm has an optimization algorithm at it’s core. Lets look at a very important yet simple optimization algorithm that you can use with any machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeD_MZanGN0h"
   },
   "source": [
    "## **Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6z-ehFwGNyO"
   },
   "source": [
    "Gradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function (f) that minimizes a cost function (cost).\n",
    "\n",
    "Gradient descent is best used when the parameters cannot be calculated analytically (e.g. using linear algebra) and must be searched for by an optimization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVAR-Q03GNtS"
   },
   "source": [
    "#### **Intuition of Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ein_FM8rGNq4"
   },
   "source": [
    "Think of a large bowl like what you would eat cereal out of or store fruit in. This bowl is a plot of the cost function (f) in 3D space.\n",
    "\n",
    "A random position on the surface of the bowl is the cost of the current values of the coefficients (cost). The bottom of the bowl is the cost of the best set of coefficients, the minimum of the function.\n",
    "\n",
    "The goal is to continue to try different values for the coefficients, evaluate their cost and select new coefficients that have a slightly better (lower) cost.\n",
    "Repeating this process enough times will lead to the bottom of the bowl and you will know the values of the coefficients that result in the minimum cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqdvFpxMGM_I"
   },
   "source": [
    "So our aim is to reach the line \n",
    "$$ y^p = mx + c $$, \n",
    "such that cost function\n",
    "$$ cost = \\sum_i (y_i - (mx_i + c))^2 $$\n",
    "is minimised.\n",
    "Here, m are the coefficents of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFp7yrIGMZXV"
   },
   "source": [
    "The graph of our cost function will look like this :\n",
    "\n",
    "<img src = \"\thttps://files.codingninjas.in/graph_gd2-7053.jpg\" width = \"400\">\n",
    "\n",
    "where, $Cost_{min}$ and $m_{min}$ are the minimum values of $Cost$ and $m$ respectively.\n",
    "\n",
    "The idea is to select $m$ and $cost$ randomly in the beginning. Then, we will find the slope.\n",
    "\n",
    "If the slope is positive, the selected $m$ is to the right of $m_{min}$.\n",
    "\n",
    "If the slope is negative, the selected $m$ is to the left of $m_{min}$.\n",
    "\n",
    "Using the slope, the new optimised value of m ($m'$) can be calculated by :\n",
    "$$ m' = m - \\alpha(slope_m) $$\n",
    "\n",
    "Also, optimised intercept $c$ can be calculated by :\n",
    "$$ c' = c - \\alpha(slope_c) $$\n",
    "\n",
    "where,\n",
    "$\\alpha$ is the learning rate. \n",
    "$$ slope_m = \\frac{\\partial Cost}{\\partial m} $$\n",
    "and \n",
    "$$ slope_c = \\frac{\\partial Cost}{\\partial c} $$\n",
    "\n",
    "It is very easy to find the above two partials.\n",
    "Taking the derivative of $Cost$ wrt $m$ gives us\n",
    "\n",
    "$$ \\frac{\\partial Cost}{\\partial m} = \\frac{-2}{N}\\sum_i(y_i - mx_i - c)x_i $$\n",
    "Taking the derivative of $Cost$ wrt $c$ gives us\n",
    "\n",
    "$$ \\frac{\\partial Cost}{\\partial c} = \\frac{-2}{N}\\sum_i(y_i - mx_i - c) $$\n",
    "\n",
    "\n",
    "Now the question arises, how many times do we optimise $m$ and $c$?\n",
    "\n",
    "For each new value of $m$ and $c$, calculate the $Cost$ too. If all is done correclty, you will notice that with each new optimised value, optimised $Cost$ will keep decreasing.\n",
    "\n",
    "So, we keep on optimising $m$ and $c$ till we reach a point, where the change is $Cost$ (ie, the decrease in cost) is very less.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKbB1RKPMZDS"
   },
   "source": [
    "#### **Learning Rate ($\\alpha$) and its Importance**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ha9lW4N_MZAx"
   },
   "source": [
    "How big the steps are gradient descent takes into the direction of the local minimum are determined by the learning rate, which figures out how fast or slow we will move towards the optimal weights.\n",
    "\n",
    "For gradient descent to reach the local minimum we must set the learning rate to an appropriate value, which is neither too low nor too high. This is important because if the steps it takes are too big, it may not reach the local minimum because it bounces back and forth between the convex function of gradient descent (see left image below). If we set the learning rate to a very small value, gradient descent will eventually reach the local minimum but that may take a while (see the right image). \n",
    "\n",
    "<img src=\"https://files.codingninjas.in/gradient-descent-learning-rate-7052.png\" width=\"550\">\n",
    "\n",
    "So, the learning rate should never be too high or too low for this reason. You can check if you’re learning rate is doing well by plotting it on a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n76YtelzOXNs"
   },
   "source": [
    "#### **Adaptive Learning Rate**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ihW18ejOXrt"
   },
   "source": [
    "The performance of the model on the training dataset can be monitored by the learning algorithm and the learning rate can be adjusted in response. \n",
    "This is called an adaptive learning rate. \n",
    "Perhaps the simplest implementation is to make the learning rate smaller once the performance of the model plateaus, such as by decreasing the learning rate by a factor of two.\n",
    "\n",
    "An adaptive learning rate method will generally outperform a model with a badly configured learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7OOJrLIhcn1"
   },
   "source": [
    "### **Let's code Gradient Descent for a Single Feature**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqriJytGh2L3"
   },
   "source": [
    "**Loading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YobHmqZNh6nb",
    "outputId": "9217a2ce-905a-4989-f4a8-f955a9cf7257"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.loadtxt(\"https://files.codingninjas.in/data-6984.csv\", delimiter=\",\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DomnCAj0pAVF",
    "outputId": "0fd4d2a1-999b-40d8-bcdd-502c8dac5502"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = data[:70,:]\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FjD-2DIppOrJ",
    "outputId": "1235619c-bb90-41ba-b4b5-90628a28f339"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data = data[70:,]\n",
    "testing_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkF9c3OGh2IM"
   },
   "source": [
    "**Now, using gradient descent, we will find the best values of m and c**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hlk8sUK_ieFz"
   },
   "outputs": [],
   "source": [
    "# This function finds the new gradient at each step\n",
    "def step_gradient(points, learning_rate, m , c):\n",
    "    m_slope = 0\n",
    "    c_slope = 0\n",
    "    M = len(points)\n",
    "    for i in range(M):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        m_slope += (-2/M)* (y - m * x - c)*x\n",
    "        c_slope += (-2/M)* (y - m * x - c)\n",
    "    new_m = m - learning_rate * m_slope\n",
    "    new_c = c - learning_rate * c_slope\n",
    "    return new_m, new_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-ceqVsJjXIy"
   },
   "outputs": [],
   "source": [
    "# The Gradient Descent Function\n",
    "def gd(points, learning_rate, num_iterations):\n",
    "    m = 0       # Intial random value taken as 0\n",
    "    c = 0       # Intial random value taken as 0\n",
    "    for i in range(num_iterations):\n",
    "        m, c = step_gradient(points, learning_rate, m , c)\n",
    "        print(i, \" Cost: \", cost(points, m, c))\n",
    "    return m, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-sIta_SjdeE"
   },
   "outputs": [],
   "source": [
    "# This function finds the new cost after each optimisation.\n",
    "def cost(points, m, c):\n",
    "    total_cost = 0\n",
    "    M = len(points)\n",
    "    for i in range(M):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        total_cost += (1/M)*((y - m*x - c)**2)\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1q4EzknjlGW"
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    learning_rate = 0.0001\n",
    "    num_iterations = 100\n",
    "    m, c = gd(training_data, learning_rate, num_iterations)\n",
    "    print(\"Final m :\", m)\n",
    "    print(\"Final c :\", c)\n",
    "    return m,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ehsYo3P3jyvL",
    "outputId": "f20ed4d2-d741-4287-bc10-bd3f2475b77b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Cost:  1461.4044104341087\n",
      "1  Cost:  460.8670268567474\n",
      "2  Cost:  205.4870778024464\n",
      "3  Cost:  140.30318108579826\n",
      "4  Cost:  123.66545280139864\n",
      "5  Cost:  119.41878332450108\n",
      "6  Cost:  118.33484209854512\n",
      "7  Cost:  118.05816441204072\n",
      "8  Cost:  117.98753491264765\n",
      "9  Cost:  117.96949772470519\n",
      "10  Cost:  117.96488434447647\n",
      "11  Cost:  117.96369729432573\n",
      "12  Cost:  117.96338479030575\n",
      "13  Cost:  117.96329550799736\n",
      "14  Cost:  117.9632632015475\n",
      "15  Cost:  117.9632454379033\n",
      "16  Cost:  117.96323138633423\n",
      "17  Cost:  117.96321828237488\n",
      "18  Cost:  117.96320542041524\n",
      "19  Cost:  117.96319262035338\n",
      "20  Cost:  117.9631798362198\n",
      "21  Cost:  117.96316705628094\n",
      "22  Cost:  117.96315427754192\n",
      "23  Cost:  117.96314149923846\n",
      "24  Cost:  117.96312872117527\n",
      "25  Cost:  117.96311594330255\n",
      "26  Cost:  117.9631031656078\n",
      "27  Cost:  117.9630903880875\n",
      "28  Cost:  117.96307761074107\n",
      "29  Cost:  117.96306483356811\n",
      "30  Cost:  117.96305205656859\n",
      "31  Cost:  117.96303927974257\n",
      "32  Cost:  117.96302650309\n",
      "33  Cost:  117.96301372661085\n",
      "34  Cost:  117.96300095030523\n",
      "35  Cost:  117.96298817417302\n",
      "36  Cost:  117.96297539821425\n",
      "37  Cost:  117.96296262242893\n",
      "38  Cost:  117.96294984681701\n",
      "39  Cost:  117.96293707137862\n",
      "40  Cost:  117.96292429611363\n",
      "41  Cost:  117.962911521022\n",
      "42  Cost:  117.96289874610383\n",
      "43  Cost:  117.96288597135916\n",
      "44  Cost:  117.96287319678783\n",
      "45  Cost:  117.96286042238998\n",
      "46  Cost:  117.96284764816549\n",
      "47  Cost:  117.9628348741145\n",
      "48  Cost:  117.96282210023688\n",
      "49  Cost:  117.9628093265326\n",
      "50  Cost:  117.96279655300184\n",
      "51  Cost:  117.96278377964448\n",
      "52  Cost:  117.96277100646049\n",
      "53  Cost:  117.96275823344999\n",
      "54  Cost:  117.96274546061275\n",
      "55  Cost:  117.96273268794901\n",
      "56  Cost:  117.96271991545866\n",
      "57  Cost:  117.96270714314169\n",
      "58  Cost:  117.96269437099812\n",
      "59  Cost:  117.9626815990279\n",
      "60  Cost:  117.96266882723108\n",
      "61  Cost:  117.96265605560774\n",
      "62  Cost:  117.9626432841577\n",
      "63  Cost:  117.96263051288103\n",
      "64  Cost:  117.9626177417778\n",
      "65  Cost:  117.9626049708479\n",
      "66  Cost:  117.96259220009142\n",
      "67  Cost:  117.96257942950821\n",
      "68  Cost:  117.96256665909843\n",
      "69  Cost:  117.96255388886206\n",
      "70  Cost:  117.96254111879902\n",
      "71  Cost:  117.96252834890933\n",
      "72  Cost:  117.962515579193\n",
      "73  Cost:  117.96250280965003\n",
      "74  Cost:  117.96249004028041\n",
      "75  Cost:  117.96247727108418\n",
      "76  Cost:  117.96246450206127\n",
      "77  Cost:  117.96245173321168\n",
      "78  Cost:  117.96243896453551\n",
      "79  Cost:  117.9624261960326\n",
      "80  Cost:  117.96241342770308\n",
      "81  Cost:  117.96240065954687\n",
      "82  Cost:  117.96238789156405\n",
      "83  Cost:  117.96237512375453\n",
      "84  Cost:  117.9623623561184\n",
      "85  Cost:  117.9623495886555\n",
      "86  Cost:  117.96233682136597\n",
      "87  Cost:  117.96232405424978\n",
      "88  Cost:  117.96231128730685\n",
      "89  Cost:  117.96229852053732\n",
      "90  Cost:  117.96228575394112\n",
      "91  Cost:  117.96227298751815\n",
      "92  Cost:  117.96226022126854\n",
      "93  Cost:  117.96224745519224\n",
      "94  Cost:  117.96223468928922\n",
      "95  Cost:  117.96222192355955\n",
      "96  Cost:  117.96220915800318\n",
      "97  Cost:  117.96219639262009\n",
      "98  Cost:  117.96218362741031\n",
      "99  Cost:  117.96217086237382\n",
      "Final m : 1.458255777804894\n",
      "Final c : 0.032397159787702676\n"
     ]
    }
   ],
   "source": [
    "m, c = run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yt6xE00th1mu"
   },
   "source": [
    "It can be seen, that the cost for the last many iterations remains almost the same, which is 117.962.\n",
    "\n",
    "For this cost, the final m is found to be 1.4582 and final c is found to be 0.0323.\n",
    "\n",
    "These optimised values may then be plugged into our hypothesis funcion to find $y_{pred}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GUpU92HpsAO"
   },
   "source": [
    "**Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lAzkZW-prFG"
   },
   "outputs": [],
   "source": [
    "def predict(final_m, final_c, testing_data):\n",
    "    y_pred = []\n",
    "    for i in range(len(testing_data)):\n",
    "        ans = m*testing_data[i][0] + c\n",
    "        y_pred.append(ans)\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N1q0jeM6qYlN",
    "outputId": "b009df9f-f840-4979-bcf1-a20b00015f4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46.095951282291885,\n",
       " 78.28376167276944,\n",
       " 68.10702680868928,\n",
       " 62.8946250628685,\n",
       " 102.61496837133335,\n",
       " 64.91436131908361,\n",
       " 83.8887150992528,\n",
       " 53.885894749919025,\n",
       " 81.41143026364702,\n",
       " 56.838414234095204,\n",
       " 83.00892226345628,\n",
       " 82.96180012666355,\n",
       " 50.09887462980676,\n",
       " 86.14202346395936,\n",
       " 84.30240868699974,\n",
       " 79.18991662796913,\n",
       " 74.5328181331299,\n",
       " 73.35763378901311,\n",
       " 64.50442501659097,\n",
       " 55.45411963583681,\n",
       " 48.06804235977706,\n",
       " 78.32854078411849,\n",
       " 100.31042647345949,\n",
       " 67.44897116945312,\n",
       " 99.65949980894587,\n",
       " 72.98918795613403,\n",
       " 71.83656946861555,\n",
       " 73.00289789301861,\n",
       " 70.24720708812089,\n",
       " 36.67615508488324]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict(m, c, testing_data)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YIbimIksCAd"
   },
   "source": [
    "### **Using the inbuilt Gradient Booster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u0nOdHNZss3L",
    "outputId": "0eeaed43-9765-411c-bc09-d1cfd5e26700"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
       "                          init=None, learning_rate=0.1, loss='ls', max_depth=3,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_iter_no_change=None, presort='deprecated',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model = GradientBoostingRegressor()\n",
    "x_train = training_data[:,0].reshape(-1, 1)\n",
    "y_train = training_data[:,1]\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S6bboIhOtTUF",
    "outputId": "c3638b67-a73d-4531-aebb-ee99285ba12b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33.05621077, 82.80773117, 73.60871863, 62.80258768, 85.242871  ,\n",
       "       62.80258768, 77.29833337, 60.4283958 , 83.40209375, 58.3415076 ,\n",
       "       83.40209375, 83.40209375, 51.3317227 , 77.88979793, 77.29833337,\n",
       "       82.80773117, 75.26029188, 72.23701306, 62.80258768, 76.2986538 ,\n",
       "       51.19104556, 82.80773117, 85.242871  , 73.60871863, 85.242871  ,\n",
       "       78.9358657 , 79.11953733, 78.9358657 , 68.91225498, 33.05621077])"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(testing_data[:,0].reshape(-1, 1))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaxOgD6BNFYL"
   },
   "source": [
    "#### **Types of Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZYJ6JdgNMHf"
   },
   "source": [
    "***Batch Gradient Descent***\n",
    "\n",
    "Batch gradient descent, also called vanilla gradient descent, calculates the error for each example within the training dataset, but only after all training examples have been evaluated does the model get updated. This whole process is like a cycle and it's called a training epoch.\n",
    "\n",
    "Some advantages of batch gradient descent are that it's computationally efficient, it produces a stable error gradient and a stable convergence. Some disadvantages are the stable error gradient can sometimes result in a state of convergence that isn’t the best the model can achieve. It also requires the entire training dataset be in memory and available to the algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pqc-83HZNVFj"
   },
   "source": [
    "***Stochastic Gradient Descent***\n",
    "\n",
    "By contrast, stochastic gradient descent (SGD) does this for each training example within the dataset, meaning it updates the parameters for each training example one by one. Depending on the problem, this can make SGD faster than batch gradient descent. One advantage is the frequent updates allow us to have a pretty detailed rate of improvement.\n",
    "\n",
    "The frequent updates, however, are more computationally expensive than the batch gradient descent approach. Additionally, the frequency of those updates can result in noisy gradients, which may cause the error rate to jump around instead of slowly decreasing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKDTcq-ENU6c"
   },
   "source": [
    "***Mini-Batch Gradient Descent***\n",
    "\n",
    "Mini-batch gradient descent is the go-to method since it’s a combination of the concepts of SGD and batch gradient descent. It simply splits the training dataset into small batches and performs an update for each of those batches. This creates a balance between the robustness of stochastic gradient descent and the efficiency of batch gradient descent.\n",
    "\n",
    "Common mini-batch sizes range between 50 and 256, but like any other machine learning technique, there is no clear rule because it varies for different applications. This is the go-to algorithm when training a neural network and it is the most common type of gradient descent within deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEWQhyBjNm0Y"
   },
   "source": [
    "#### **Some Useful Tips for Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZ9xlvRxNr5s"
   },
   "source": [
    "**Plot Cost versus Time:** Collect and plot the cost values calculated by the algorithm each iteration. The expectation for a well performing gradient descent run is a decrease in cost each iteration. If it does not decrease, try reducing your learning rate.\n",
    "\n",
    "**Choose correct Learning Rate:** The learning rate value is a small real value such as 0.1, 0.001 or 0.0001. Try different values for your problem and see which works best.\n",
    "\n",
    "**Rescale Inputs:** The algorithm will reach the minimum cost faster if the shape of the cost function is not skewed and distorted. You can achieved this by rescaling all of the input variables (X) to the same range, such as [0, 1] or [-1, 1].\n",
    "\n",
    "**Few Passes:** Stochastic gradient descent often does not need more than 1-10 passes through the training dataset to converge on good or good enough coefficients.\n",
    "\n",
    "**Plot Mean Cost:** The updates for each training dataset instance can result in a noisy plot of cost over time when using stochastic gradient descent. Taking the average over 10, 100, or 1000 updates can give you a better idea of the learning trend for the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5HyDcjRPeJG"
   },
   "source": [
    "#### **Your Next Task**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cq4rvP-qPiyg"
   },
   "source": [
    "You have learnt how to code Gradient Descent for a single featured dataset. Try to code a more Generic Gradient Descent. Let us consider that the $i^{th}$ feature for the first row is $x_1^i$. Similarily for the $j^th$ row, the $i^{th}$ feature will be $x_j^i$.So, your cost function would look something like :\n",
    "\n",
    "$$ cost = \\frac{1}{M}\\sum_i^M (y_i - (m_ix_i^1 + m_ix_i^2 + m_ix_i^3 + ...... + m_{n + 1}x_{n + 1} ))^2 $$\n",
    "\n",
    "Here $m_{n + 1}x_{n + 1}$ is actually 'c', constant value. (We usually take them to be 1)\n",
    "\n",
    "Also, to find the next m (m'), our equation becomes :\n",
    "$$ m_j' = m_j - \\alpha\\frac{\\partial cost}{\\partial m_j} $$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\frac{\\partial cost}{\\partial m_i} = \\frac{1}{M}\\sum_i^M 2(y_i - (m_ix_i^1 + m_ix_i^2 + m_ix_i^3 + ...... + m_{n + 1}x_{n + 1} ))x_i^j $$"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of MultiVariable Regression and Gradient Descent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
